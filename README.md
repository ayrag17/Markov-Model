# Markov Model

A Python implementation of a **Markov Chain** model, used to simulate random processes and predict future events based on the current state, governed by transition probabilities. This project includes a text generation feature using a Markov Chain, where the generated text is based on a given **text sample**.

The model creates a dictionary where each key represents a word, and the value is a list of words that can follow it, along with the probabilities of those transitions. This structure is then used to generate new text in the style of the input text.

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [How It Works](#how-it-works)
- [License](#license)

## Overview

This project provides a **Markov Chain** implementation in Python using a dictionary-based approach. Given an input text, it builds a transition matrix in the form of a dictionary, where each key is a word, and the corresponding value is another dictionary representing the words that can follow it, along with their respective probabilities.

Using this transition matrix, new text can be generated by selecting words based on the probabilities of their appearance after the current word.

## Features

- **Transition Dictionary**: A dictionary-based transition matrix that represents word probabilities.
- **Text Generation**: Generate random text that mimics the style and structure of the input text.
- **Customizable Input**: Provide any text sample to generate new content based on its structure.

## How It Works

### Transition Dictionary

The key idea behind this Markov Chain implementation is the use of a **transition dictionary** to store the probabilities of words following other words in the input text. This is how it works:

1. **Split the Input Text**: The input text is split into individual words.
2. **Track Word Transitions**: For each word in the text, the model tracks the next word that follows it.
3. **Count Word Pairs**: The model counts how often each word is followed by another word.
4. **Calculate Probabilities**: The counts are then converted into probabilities, where the probability of a word following another word is determined by the frequency of the pair compared to the total number of transitions for the current word.

This dictionary structure effectively maps the relationships between words in the input text.

### Text Generation

Once the transition dictionary is built, generating new text involves the following steps:

1. **Starting Word**: Begin with a randomly selected word or a user-provided starting word.
2. **Transition Probabilities**: Look up the possible next words for the current word in the dictionary. Each possible next word has an associated probability based on how frequently it follows the current word in the input text.
3. **Random Selection**: Randomly select the next word based on the transition probabilities.
4. **Repeat**: Continue this process to generate a sequence of words (a sentence or more), stopping when no further transitions are possible.

This method generates **plausible-sounding** text that mimics the style and structure of the input text. The process is memoryless, meaning each word is selected based only on the current word and the transition probabilities, not any of the previous words.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
